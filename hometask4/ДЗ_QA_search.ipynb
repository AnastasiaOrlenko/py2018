{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ДЗ по поиску\n",
    "\n",
    "Привет! Вам надо реализивать поисковик на базе вопросов-ответов с сайта [pravoved.ru](https://pravoved.ru/questions-archive/).        \n",
    "Поиск должен работать на трех технологиях:       \n",
    "1. обратном индексе     \n",
    "2. word2vec         \n",
    "3. doc2vec      \n",
    "\n",
    "Вы должны понять, какой метод и при каких условиях эксперимента на этом корпусе работает лучше.          \n",
    "Для измерения качества поиска найдите точность (accuracy) выпадания правильного ответа на конкретный вопрос (в этой базе у каждого вопроса есть только один правильный ответ). Точность нужно измерить для всей базы.    \n",
    "При этом давайте считать, что выпал правильный ответ, если он попал в **топ-5** поисковой выдачи.\n",
    "\n",
    "> Сделайте ваш поиск максимально качественным, чтобы значение точности стремилось к 1.     \n",
    "Для этого можно поэкспериментировать со следующим:       \n",
    "- модель word2vec (можно брать любую из опен сорса или обучить свою)\n",
    "- способ получения вектора документа через word2vec: простое среднее арифметическое или взвешивать каждый вектор в соответствии с его tf-idf      \n",
    "- количество эпох у doc2vec (начинайте от 100)\n",
    "- предобработка документов для обучения doc2vec (удалять / не удалять стоп-слова)\n",
    "- блендинг методов поиска: соединить результаты обратного индекса и w2v, или (что проще) w2v и d2v\n",
    "\n",
    "На это задание отведем 10 дней. Дэдлайн сдачи до полуночи 12.10."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "with open('qa_corpus.pkl', 'rb') as file:\n",
    "    qa_corpus = pickle.load(file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Всего в корпусе 1384 пары вопрос-ответ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "1384"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(qa_corpus)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Первый элемент блока это вопрос, второй - ответ на него"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['\\nДобрый день.Мой сын гражданин Украины (ДНР),имеет вид на жительство в Р.Ф., кот.получил проживая с 2014 г. в Нижегородской области.В 2017г. переехал на постоянное место жительство в г.Ростов.Официально трудоустроился на одно из промышл.предприятий г.Ростова.Оформил временную регистрацию в Ростове.В УФМС предупредили,что по истечении 90 дней он должен либо постоянно прописаться либо покинуть территорию России.Прошу проконсультировать как быть дальше.(Вернуться домой в Донецк,но здесь идет война,работы нет.В Ростове он работает по специальности.Он инженер машиностроитель.)Временная прописка до 15 марта.  Если он сможет приобрести какую либо недвижимость,как долго будет решаться вопрос о его постоянной прописке в Ростове.Как в этом случае будет решаться вопрос с видом на жительство в Ростове? Не получится ли ,что приобретя квартиру,он не успеет в ней прописаться до окончании срока временной регистрации. С уважением Людмила Евгеньевна.\\n',\n",
       " 'Добрый вечер!Из Вашего вопроса вообще ничего не ясно.Ваш сын по ВНЖ в Нижегородской обл. сделал временную\\xa0 на 90 дней в Ростове? Так? Или в чем заключается вопрос?С ув., АлёнаМиграционный юристРостов-на-Дону ']"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "qa_corpus[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Файлы до моделей"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec, KeyedVectors\n",
    "import codecs\n",
    "import json\n",
    "w2v_path = 'models/ru.bin'\n",
    "d2v_path = 'models/qa_d2v.bin'\n",
    "bm25_path = 'models/qa_bm25.bin'\n",
    "\n",
    "models = {}\n",
    "vectors = {}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Предобработка базы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done preprocessing\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "import pymorphy2\n",
    "\n",
    "w = re.compile('[A-zА-я]+')\n",
    "morph = pymorphy2.MorphAnalyzer()\n",
    "\n",
    "def get_words(text):\n",
    "    words = w.findall(text)\n",
    "    words = [morph.parse(w.lower())[0].normal_form for w in words]\n",
    "    return words\n",
    "\n",
    "def count_words(text):\n",
    "    words = {}\n",
    "    indoc = get_words(text)\n",
    "    for word in indoc:\n",
    "        word = word.lower()\n",
    "        if word not in words:\n",
    "            words[word] = 0\n",
    "        words[word] = words[word] + 1\n",
    "    return words\n",
    "\n",
    "def preprocessing(docs):\n",
    "    return [count_words(doc[0]) for doc in docs]\n",
    "\n",
    "processed_qa = preprocessing(qa_corpus)\n",
    "print('done preprocessing')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение word2vec модели и базы векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, 1384 vectors\n"
     ]
    }
   ],
   "source": [
    "def get_w2v_vector(words):\n",
    "    vec = [0 for i in range(models['w2v'].vector_size)]\n",
    "    size = 0\n",
    "    for word, count in words.items():\n",
    "        if word not in models['w2v'].wv:\n",
    "            continue\n",
    "        size += count\n",
    "        wv = models['w2v'].wv[word]\n",
    "        vec = [vec[i] + wv[i] * count for i in range(len(vec))]\n",
    "    if size == 0:\n",
    "        return None\n",
    "    return [i / size for i in vec]\n",
    "        \n",
    "def get_w2v_vectors(docs):\n",
    "    vectors = []\n",
    "    for index, doc in enumerate(docs):\n",
    "        vec = get_w2v_vector(doc)\n",
    "        if vec:\n",
    "            vectors.append((index, vec))\n",
    "    \n",
    "    return vectors\n",
    "\n",
    "def save_w2v_base(vectors):\n",
    "    cpy = []\n",
    "    for vec in vectors:\n",
    "        cpy.append((vec[0], [round(i, 6) for i in vec[1]]))\n",
    "    file = codecs.open('qa_w2v_base.bin', 'w', 'utf-8')\n",
    "    file.write(json.dumps(cpy, indent=1))\n",
    "    file.flush()\n",
    "    file.close()\n",
    "\n",
    "models['w2v'] = Word2Vec.load(w2v_path)\n",
    "vectors['w2v'] = get_w2v_vectors(processed_qa)\n",
    "save_w2v_base(vectors['w2v'])\n",
    "print('done, ' + str(len(vectors['w2v'])) + ' vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение doc2vec модели и базы векторов"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done, 1384 vectors\n"
     ]
    }
   ],
   "source": [
    "from gensim.models.doc2vec import TaggedDocument, Doc2Vec\n",
    "\n",
    "def train_doc2vec(docs):\n",
    "    tagged = []\n",
    "    for index, doc in enumerate(docs):\n",
    "        td = TaggedDocument(words=list(doc.keys()), tags=[index])\n",
    "        tagged.append(td)\n",
    "\n",
    "    d2v_model = Doc2Vec(vector_size=100, alpha=0.025, min_count=2, dm=1)\n",
    "    d2v_model.build_vocab(tagged)\n",
    "    for epoch in range(100):\n",
    "        d2v_model.train(tagged,\n",
    "                    total_examples=d2v_model.corpus_count,\n",
    "                    epochs=d2v_model.epochs)\n",
    "        # decrease the learning rate\n",
    "        d2v_model.alpha -= 0.0002\n",
    "        # fix the learning rate, no decay\n",
    "        d2v_model.min_alpha = d2v_model.alpha\n",
    "    \n",
    "    d2v_model.save(d2v_path)\n",
    "    return d2v_model\n",
    "\n",
    "def get_d2v_vectors(docs):\n",
    "    vectors = []\n",
    "    for index, doc in enumerate(docs):\n",
    "        vec = models['d2v'].infer_vector(list(doc.keys()), epochs=10)\n",
    "        vec = [float(f) for f in vec]\n",
    "        vectors.append((index, vec))\n",
    "    return vectors\n",
    "\n",
    "def save_d2v_base(vectors):\n",
    "    cpy = []\n",
    "    for vec in vectors:\n",
    "        cpy.append((vec[0], [round(i, 6) for i in vec[1]]))\n",
    "    file = codecs.open('qa_d2v_base.bin', 'w', 'utf-8')\n",
    "    file.write(json.dumps(cpy, indent=1))\n",
    "    file.flush()\n",
    "    file.close()\n",
    "\n",
    "#models['d2v'] = Doc2Vec.load(d2v_path)\n",
    "models['d2v'] = train_doc2vec(processed_qa)\n",
    "vectors['d2v'] = get_d2v_vectors(processed_qa)\n",
    "save_d2v_base(vectors['d2v'])\n",
    "print('done, ' + str(len(vectors['d2v'])) + ' vectors')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Получение bm25 базы"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "from bm25.query import QueryProcessor\n",
    "from bm25.parse import Corpus\n",
    "\n",
    "def build_inv_index(docs):\n",
    "    corp = Corpus()\n",
    "    for index, doc in enumerate(docs):\n",
    "        corp.add(index, list(doc.keys()))\n",
    "    \n",
    "    proc = QueryProcessor(corp)\n",
    "    proc.save(bm25_path)\n",
    "    return proc\n",
    "\n",
    "models['bm25'] = build_inv_index(processed_qa)\n",
    "print('done')\n",
    "#models['bm25'] = QueryProcessor.load(bm25_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Функции поиска по базам"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "from gensim import matutils\n",
    "import numpy as np \n",
    "\n",
    "def similarity(v1, v2):\n",
    "    v1_norm = matutils.unitvec(np.array(v1))\n",
    "    v2_norm = matutils.unitvec(np.array(v2))\n",
    "    return np.dot(v1_norm, v2_norm)\n",
    "\n",
    "def search_w2v(document):    \n",
    "    vec = get_w2v_vector(count_words(document))\n",
    "    results = [(i, similarity(w2v_vec[1], vec)) for i, w2v_vec in enumerate(vectors['w2v'])]\n",
    "    results.sort(key = lambda x: x[1], reverse=True)\n",
    "    return results\n",
    "\n",
    "def search_d2v(document):\n",
    "    vec = models['d2v'].infer_vector(get_words(document), epochs=10)\n",
    "    vec = [float(f) for f in vec]\n",
    "    results = [(i, similarity(d2v_vec[1], vec)) for i, d2v_vec in enumerate(vectors['d2v'])]\n",
    "    results.sort(key = lambda x: x[1], reverse=True)\n",
    "    return results\n",
    "\n",
    "def search_inv_index(document):\n",
    "    results = models['bm25'].run_query(get_words(document))\n",
    "    results = [(int(key),val) for key, val in results.items()]\n",
    "    results.sort(key = lambda x: x[1], reverse=True)\n",
    "    return results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python3.5/dist-packages/gensim/matutils.py:737: FutureWarning: Conversion of the second argument of issubdtype from `int` to `np.signedinteger` is deprecated. In future, it will be treated as `np.int64 == np.dtype(int).type`.\n",
      "  if np.issubdtype(vec.dtype, np.int):\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "iterations: 50\n",
      "correct by w2v: 100.0 %\n",
      "correct by d2v: 46.0 %\n",
      "correct by ind: 100.0 %\n"
     ]
    }
   ],
   "source": [
    "correct = [0,0,0,0]\n",
    "max_iter = 50\n",
    "for index, doc in enumerate(qa_corpus):\n",
    "    if index >= max_iter:\n",
    "        break\n",
    "    w2v_results = search_w2v(doc[0])[0:5]\n",
    "    d2v_results = search_d2v(doc[0])[0:5]\n",
    "    ind_results = search_inv_index(doc[0])\n",
    "    \n",
    "    if len(ind_results) > 5:\n",
    "        ind_results = ind_results[0:5]\n",
    "    is_found = [False, False, False]\n",
    "    for i in range(0, 5):\n",
    "        if w2v_results[i][0] == index:\n",
    "            is_found[0] = True\n",
    "        if d2v_results[i][0] == index:\n",
    "            is_found[1] = True\n",
    "        if ind_results[i][0] == index:\n",
    "            is_found[2] = True\n",
    "    for i in range(0,3):\n",
    "        if is_found[i]:\n",
    "            correct[i] += 1\n",
    "    if len([a for a in is_found if a]) > 1:\n",
    "        correct[3] += 1\n",
    "        \n",
    "print('iterations:', max_iter)\n",
    "correct = [i / max_iter * 100 for i in correct]\n",
    "print('correct by w2v:', correct[0], '%')\n",
    "print('correct by d2v:', correct[1], '%')\n",
    "print('correct by ind:', correct[2], '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
