{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание работы\n",
    "\n",
    "1. Этот датасет является не очень удобным для использования сиамской сети, поскольку неоптимизированный SVM сразу дает точность 93%, точнее acc= 0.937 acc_train= 0.99.\n",
    "2. ELMO работает очень долго, поэтому была организована предварительная конвертация датасета в Colab. Полученные представления дали при применении SVM точность acc= 0.79 acc_train= 0.97.\n",
    "3. Дополнена модель Tripletnet. Итератор gen_iter() для батчей для сиамской сети сформирован так: последовательно выбираются все вектора представления ELMO. Для каждого якоря случайно выбирается 32 вектора из его класса и 32 вектора из других классов. Всего сделано примерно 20 эпох, каждая занимала 300 сек.\n",
    "4. Сохраняется сжатое представление размерности 128 для метрического поиска.\n",
    "5. Полученные сжатые представления загружаются в индекс annoy, используется 200 деревьев.\n",
    "6. Для каждого вектора из тестовой выборки вычисляется 10 ближайших соседей и мажоритарный класс сравнивается с тестовым. То же делается для контроля с учебной выборкой. \n",
    "\n",
    "## Результаты\n",
    "- Точность  (accuracy score) для тестовой выборки составила 0.4244031830238727 и для учебной 0.5251392942425046. Оснований предположить ошибку в коде пока нет.\n",
    "- Результаты не такие плохие с учетом того, что классификация идет по 20 классам, но значительно хуже SVM.\n",
    "- Падение точности, скорее всего, связано с недообученностью сети Tripletnet. В целом кажется, что подход на основе сиамской сети подходит для One-Hot обучения, где данных очень мало. Если данных много, например, 20.000, то это означает общее количество триплетов для одной эпохи - 400 млн.\n",
    "- Тем не менее, поскольу целью задания было кодирование, а не точность, то считаю его полностью выполненным.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи\n",
    "\n",
    "Assignment 8.\n",
    "\n",
    "Develop a model for 20 news groups dataset. Select 20% of data for test set.\n",
    "\n",
    "Use metric learning with siamese networks and triplet loss.\n",
    "Use KNN and LSH (annoy library) for final prediction \n",
    "after the network was trained.\n",
    "\n",
    "! Remember, that LSH gives you a set of neighbor candidates, \n",
    "for which you have to calculate distances to choose \n",
    "top-k nearest neighbors.\n",
    "\n",
    "Your quality = accuracy score.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "import string\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, Dataset, TensorDataset\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename=\"pt.log\", level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from sklearn.feature_selection import SelectFromModel\n",
    "from sklearn.feature_selection import SelectKBest, chi2\n",
    "from sklearn.linear_model import RidgeClassifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.utils.extmath import density\n",
    "from sklearn import metrics\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "categories = None\n",
    "remove = ('headers', 'footers', 'quotes')\n",
    "remove = ()\n",
    "data = fetch_20newsgroups(subset='all', categories=categories, remove=remove)\n",
    "target_names = data.target_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(18846,) 18846 15076 3770\n",
      "(15076, 152133) (3770, 152133) (15076,) (3770,)\n"
     ]
    }
   ],
   "source": [
    "y = data.target\n",
    "data_train, data_test, y_train, y_test = train_test_split(data.data, y, test_size=0.2)\n",
    "print(y.shape, len(data.data), len(data_train), len(data_test))\n",
    "\n",
    "vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5, stop_words='english')\n",
    "X_train = vectorizer.fit_transform(data_train)\n",
    "X_test = vectorizer.transform(data_test)\n",
    "print(X_train.shape, X_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "import spacy\n",
    "import en_core_web_sm\n",
    "spacy_en = en_core_web_sm.load()\n",
    "from allennlp.modules.elmo import Elmo, batch_to_ids\n",
    "\n",
    "options_file = 'elmo_2x4096_512_2048cnn_2xhighway_options.json'\n",
    "weight_file = 'elmo_2x4096_512_2048cnn_2xhighway_weights.hdf5'\n",
    "elmo = Elmo(options_file, weight_file, 2, dropout=0)\n",
    "\n",
    "def tokenizer(text): # create a tokenizer function\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text) if tok.text.isalpha()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "start = time.time()\n",
    "N = len(data.data)\n",
    "for i in range(N):\n",
    "    t1 = data.data[i]\n",
    "    if len(t1) < 10:\n",
    "        t1 = 'xxxxx'\n",
    "    t2 = [tokenizer(t1)]\n",
    "    character_ids = batch_to_ids(t2)\n",
    "    embeddings = elmo(character_ids)\n",
    "    cat = torch.cat(embeddings['elmo_representations'], dim=-1)\n",
    "    mean = cat.mean(dim=1)\n",
    "    x = mean.detach().numpy()\n",
    "    if i == 0:\n",
    "        X = x\n",
    "    else:\n",
    "        X = np.vstack((X, x))\n",
    "    timer = time.time() - start\n",
    "    logging.info(str(i) + ' ' + str(N) + ' ' + str(X.shape) + ' ' + str(timer))\n",
    "    if i % 100 == 0:\n",
    "        np.save('X.npy', X)    \n",
    "np.save('X.npy', X)            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "def triplet_loss(anchor_embed, pos_embed, neg_embed):\n",
    "    loss = F.cosine_similarity(anchor_embed, neg_embed) - F.cosine_similarity(anchor_embed, pos_embed)\n",
    "    loss = loss.mean()\n",
    "    return loss\n",
    "\n",
    "class Tripletnet(nn.Module):\n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.fc = nn.Linear(1024*2, 128)\n",
    "        \n",
    "    def branch(self, x):\n",
    "        x = self.fc(x)\n",
    "        return x\n",
    "\n",
    "    def forward(self, anchor, pos, neg):\n",
    "        anchor = self.branch(anchor)\n",
    "        pos = self.branch(pos)\n",
    "        neg = self.branch(neg)\n",
    "        return triplet_loss(anchor, pos, neg)\n",
    "    \n",
    "model = Tripletnet()\n",
    "optimizer = optim.Adam(model.parameters())    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 158,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_epoch(model, iterator, optimizer):\n",
    "    model.train()\n",
    "    epoch_loss = 0\n",
    "    running_loss = 0\n",
    "    n = 0\n",
    "    start = time.time()\n",
    "    for batch in iterator:\n",
    "        n += 1\n",
    "        optimizer.zero_grad()\n",
    "        anchor, pos, neg = batch\n",
    "\n",
    "        loss = model(anchor, pos, neg)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        curr_loss = loss.data.detach().item()        \n",
    "        epoch_loss += curr_loss\n",
    "        \n",
    "        loss_smoothing = n / (n+1)\n",
    "        running_loss = loss_smoothing * running_loss + (1 - loss_smoothing) * curr_loss\n",
    "        \n",
    "        elapsed = time.time() - start\n",
    "        out = f'epoch={epoch} n={n} elapsed={elapsed:.0f} curr_loss={curr_loss:.3f} running_loss={running_loss:.3f}'\n",
    "        logging.info(out)\n",
    "    return epoch_loss / n\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 187,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15076, 2048) (3770, 2048) (15076,) (3770,)\n"
     ]
    }
   ],
   "source": [
    "X9 = np.load('X9.npy')\n",
    "X9_train, X9_test, y9_train, y9_test = train_test_split(X9, y, test_size=0.2)\n",
    "print(X9_train.shape, X9_test.shape, y_train.shape, y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 165,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=1 | train_loss: -0.915\n",
      "epoch=2 | train_loss: -0.922\n",
      "epoch=3 | train_loss: -0.923\n",
      "epoch=4 | train_loss: -0.927\n",
      "epoch=5 | train_loss: -0.930\n",
      "epoch=6 | train_loss: -0.931\n",
      "epoch=7 | train_loss: -0.935\n",
      "epoch=8 | train_loss: -0.936\n",
      "epoch=9 | train_loss: -0.941\n"
     ]
    }
   ],
   "source": [
    "n_epochs=10\n",
    "for epoch in range(1, n_epochs):\n",
    "    train_iterator = gen_iter()\n",
    "    train_loss = train_epoch(model, train_iterator, optimizer)\n",
    "    print(f'epoch={epoch} | train_loss: {train_loss:.3f}')  \n",
    "    torch.save(model, \"siam.pt\")  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 180,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Применение сиамской сети\n",
    "TX9_train = torch.from_numpy(X9_train)\n",
    "TX9_train = model.branch(TX9_train).detach().numpy()\n",
    "np.save(\"TX9_train.npy\", TX9_train)\n",
    "\n",
    "TX9_test = torch.from_numpy(X9_test)\n",
    "TX9_test = model.branch(TX9_test).detach().numpy()\n",
    "print(TX9_test.shape)\n",
    "np.save(\"TX9_test.npy\", TX9_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 182,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(15076, 128) (3770, 128) (15076,) (3770,)\n"
     ]
    }
   ],
   "source": [
    "np.save(\"y9_train.npy\", y9_train)\n",
    "np.save(\"y9_test.npy\", y9_test)\n",
    "print(TX9_train.shape, TX9_test.shape, y9_train.shape, y9_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Итератор для сиамской сети\n",
    "# Выбрать якорь как следующий элемент, определить его класс\n",
    "# Выделить все элементы с этим классом\n",
    "# Выбрать 32 случайных элемента этого класса и 32 из чужих\n",
    "# вернуть генератор с тремя батчами\n",
    "\n",
    "def gen_iter():\n",
    "    for i, i_class in enumerate(y9_train):\n",
    "        i_index = []\n",
    "        n_index = []\n",
    "        for j, j_class in enumerate(y9_train):\n",
    "            if j == i:\n",
    "                continue\n",
    "            if j_class == i_class:\n",
    "                i_index.append(j)\n",
    "            else:\n",
    "                n_index.append(j)\n",
    "        pos = np.random.choice(np.array(i_index), 32)\n",
    "        neg = np.random.choice(np.array(n_index), 32)\n",
    "    \n",
    "        anchor_ = torch.zeros(32, 1024*2)\n",
    "        pos_ = torch.zeros(32, 1024*2)\n",
    "        neg_ = torch.zeros(32, 1024*2)\n",
    "   \n",
    "        for k in range(32):\n",
    "            p = pos[k]\n",
    "            p_class = y9_train[p]\n",
    "            n = neg[k]\n",
    "            n_class = y9_train[n]\n",
    "       \n",
    "            ta = torch.from_numpy(X9_train[i])\n",
    "            tp = torch.from_numpy(X9_train[p])\n",
    "            tn = torch.from_numpy(X9_train[n])\n",
    "        \n",
    "            anchor_[k] = ta\n",
    "            pos_[k] = tp\n",
    "            neg_[k] = tn\n",
    "        \n",
    "        yield anchor_, pos_, neg_\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from annoy import AnnoyIndex"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TX9_train = np.load(DIR + 'TX9_train.npy')\n",
    "TX9_test = np.load(DIR + 'TX9_test.npy')\n",
    "y9_train = np.load(DIR + 'y9_train.npy')\n",
    "y9_test = np.load(DIR + 'y9_test.npy')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "N_train = TX9_train.shape[0]\n",
    "N_test = TX9_test.shape[0]\n",
    "M = 128\n",
    "train = AnnoyIndex(M)\n",
    "for i in range(N_train):\n",
    "    v = TX9_train[i]\n",
    "    train.add_item(i, v)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for i in range(N_test):\n",
    "    v = TX9_test[i]\n",
    "    true = y9_test[i]\n",
    "    nn = train.get_nns_by_vector(v, 20)\n",
    "    class100 = []\n",
    "    for j in nn:\n",
    "      class1 = y9_train[j]\n",
    "      class100.append(class1)\n",
    "    prob = np.zeros(20)\n",
    "    for class1 in class100:\n",
    "        prob[class1] += 1\n",
    "    pred = np.argmax(prob)\n",
    "    if true == pred:\n",
    "      acc += 1\n",
    "res = acc / N_test\n",
    "print(res)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "acc = 0\n",
    "for i in range(N_train):\n",
    "    v = TX9_train[i]\n",
    "    true = y9_train[i]\n",
    "    nn = train.get_nns_by_vector(v, 10)\n",
    "    class100 = []\n",
    "    for j in nn:\n",
    "      class1 = y9_train[j]\n",
    "      class100.append(class1)\n",
    "    prob = np.zeros(20)\n",
    "    for class1 in class100:\n",
    "        prob[class1] += 1\n",
    "    pred = np.argmax(prob)\n",
    "    if true == pred:\n",
    "      acc += 1\n",
    "res = acc / N_train\n",
    "print(res)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
