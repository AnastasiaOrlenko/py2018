{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Описание работы\n",
    "\n",
    "## Результаты\n",
    " * Сеть построена и обучена, выполнены все другие условия.\n",
    " * Для генерации выбрана длина последовательности - 10 (также использовались 20, 30) и начальный токен - заглавная буква. Примеры в 629.\n",
    " * Построенная отдельно вручную т-гр. модель дает значение перплексии 9.3 для 2-гр, 9.1 для 3-гр, 6.1 для 4-гр. Поэтому результаты текущей модели как 7.4 можно считать неплохими."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Постановка задачи\n",
    "\n",
    "Assignment 7.\n",
    "Delelop language model, which generates death metal band names.\n",
    "You can get data from https://www.kaggle.com/zhangjuefei/death-metal.\n",
    "You are free to use any other data, but the most easy way is just to take the band name column. \n",
    "Your language model should be char-based autogression RNN.\n",
    "Text generation should be terminated when either max length is reached or terminal symbol is generated.\n",
    "\n",
    "Different band names can be generated by:\n",
    " - init $h_0$ as random vector from some probabilty distribution.\n",
    " - sampling over tokens at each timestep with probability = softmax\n",
    "\n",
    "Calculate perplexity for your model = your objective quality metric.\n",
    "Also, sample 10 band names from your model for subjective evaluation. E.g. names like 'qwiouefiou23riop2h3' or 'death death death!' are bad examples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import random\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "\n",
    "SEED = 1234\n",
    "torch.manual_seed(SEED)\n",
    "torch.cuda.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "\n",
    "import time\n",
    "import string\n",
    "import random\n",
    "\n",
    "import logging\n",
    "logging.basicConfig(filename=\"pt.log\", level=logging.INFO)\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "%matplotlib inline\n",
    "import seaborn as sns\n",
    "from pylab import rcParams\n",
    "rcParams['figure.figsize'] = 10, 8\n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "\n",
    "import unicodedata\n",
    "from sklearn.utils import shuffle"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(37723,)"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = pd.read_csv('bands.csv')\n",
    "dn = shuffle(df['name'])\n",
    "dn.to_csv('names.csv', index=False)\n",
    "bb = dn.tolist()\n",
    "dn.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Удалены русские, китайские и другие не ascii имена - всего 1.8%\n",
    "i = 0\n",
    "k = 0\n",
    "bc = []\n",
    "s = set()\n",
    "max = 0\n",
    "mm = []\n",
    "for b in bb:\n",
    "    i += 1\n",
    "    benc = b.encode('ascii', 'ignore').decode('utf8')\n",
    "    if benc != b:\n",
    "        k += 1\n",
    "        #print(b, '=', benc)    \n",
    "        continue\n",
    "    bc.append(b)\n",
    "    s |= set(b)\n",
    "    lena = len(b)\n",
    "    mm.append(lena)\n",
    "    if lena > max:\n",
    "        max = lena\n",
    "print(i, k, len(bb), len(bc), len(s), max, k*100/i)\n",
    "print(max, np.mean(mm), np.std(mm))\n",
    "\n",
    "# Подготовка\n",
    "characters = tuple(s)\n",
    "int2char = dict(enumerate(characters))\n",
    "char2int = {char: index for index, char in int2char.items()}\n",
    "vocab_size = len(char2int)\n",
    "print(vocab_size)\n",
    "\n",
    "train_size = int(len(bc) * 0.8)\n",
    "print(train_size)\n",
    "b_train = bc[:train_size]\n",
    "b_test = bc[train_size:]\n",
    "print(len(b_train), len(b_test))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CharRNN(nn.Module):\n",
    "    def __init__(self, seq_len, vocab_size, embed_size, hidden_size, batch_size, n_layers=1):\n",
    "        super().__init__()\n",
    "        self.seq_len = seq_len        \n",
    "        self.vocab_size = vocab_size        \n",
    "        self.embed_size = embed_size        \n",
    "        self.hidden_size = hidden_size\n",
    "        self.batch_size = batch_size        \n",
    "        self.n_layers = n_layers\n",
    "\n",
    "        self.embedding = nn.Embedding(self.vocab_size, self.embed_size)\n",
    "        self.rnn = nn.GRU(input_size=self.embed_size, hidden_size=self.hidden_size, batch_first=True)\n",
    "        self.fc = nn.Linear(in_features=self.hidden_size, out_features=self.vocab_size)\n",
    "        \n",
    "    def forward(self, input, hidden):\n",
    "        embedded = self.embedding(input)\n",
    "        output, hidden = self.rnn(embedded, hidden)\n",
    "        output = self.fc(output)\n",
    "        return output, hidden\n",
    "    \n",
    "    def init_hidden(self):        \n",
    "        return torch.zeros(self.n_layers, self.batch_size, self.hidden_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 629,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Diggod    \n",
      "Dramial Pl\n",
      "Senton Mol\n",
      "Abril Crah\n",
      "Claore    \n",
      "Mentrechza\n",
      "The Swumen\n",
      "Blessmata \n",
      "Evil Prive\n",
      "Insanic De\n",
      "Panistiece\n",
      "Necrobes  \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "7.463027735470693"
      ]
     },
     "execution_count": 629,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def train(model, iterator, optimizer, criterion):\n",
    "    epoch_loss = 0\n",
    "    model.train()\n",
    "    for x_train, y_train in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        hidden = model.init_hidden()\n",
    "        output, hidden = model(x_train, hidden)\n",
    "        out = output.contiguous().view(batch_size * seq_len, vocab_size)\n",
    "        y = y_train.contiguous().view(batch_size * seq_len)\n",
    "        loss = criterion(out, y)\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        epoch_loss += loss.item()\n",
    "    return epoch_loss / i\n",
    "\n",
    "def perplexity(model, iterator):\n",
    "    model.eval()    \n",
    "    pp = 0\n",
    "    MIN = 1e-15\n",
    "    full = np.full((N, vocab_size), MIN)\n",
    "    softmax = nn.Softmax(dim=2)\n",
    "    for x_test, y_test in iterator:\n",
    "        hidden = model.init_hidden()\n",
    "        output, hidden = model(x_test, hidden)\n",
    "        output = softmax(output)\n",
    "        x = output.detach().numpy()\n",
    "        ind = y_test.numpy()\n",
    "        ppb = 0\n",
    "        for b in range(batch_size):\n",
    "            for s in range(seq_len):\n",
    "                k = ind[b,s]\n",
    "                prob = x[b,s,k]\n",
    "                ppb += prob * np.log(prob) \n",
    "        ppb = ppb / batch_size\n",
    "        pp += ppb\n",
    "    pp = np.exp(-1 * pp / i)\n",
    "    return pp\n",
    "    \n",
    "def generate(model, prime_str='D'):\n",
    "    pred = prime_str\n",
    "    for i in range(1, seq_len):\n",
    "        name = neq(pred, seq_len) \n",
    "        x = np.array([char2int[char] for char in name])\n",
    "        x = np.expand_dims(x, axis=0)\n",
    "        x = torch.from_numpy(x).type(torch.LongTensor)\n",
    "        hidden = torch.zeros(1, 1, hidden_size)\n",
    "        y, hidden = model(x, hidden)\n",
    "        preds = y[0, i - 1].detach().numpy()\n",
    "        exp_preds = np.exp(preds)\n",
    "        preds = exp_preds / (np.sum(exp_preds) * 1.00001)\n",
    "        probas = np.random.multinomial(1, preds, 1)\n",
    "        ind = np.argmax(probas)\n",
    "        char = int2char[ind]\n",
    "        pred = pred + char\n",
    "    print(pred)\n",
    "    \n",
    "\n",
    "generate(model, 'D')    \n",
    "generate(model, 'D')    \n",
    "generate(model, 'S')    \n",
    "generate(model, 'A')    \n",
    "generate(model, 'C')    \n",
    "generate(model, 'M')    \n",
    "generate(model, 'T')    \n",
    "generate(model, 'B')    \n",
    "generate(model, 'E')    \n",
    "generate(model, 'I')    \n",
    "generate(model, 'P')    \n",
    "generate(model, 'N')    \n",
    "\n",
    "test_iterator = iter_names(b_test, batch_size, seq_len, vocab_size)\n",
    "perplexity(model, test_iterator)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "metadata": {},
   "outputs": [],
   "source": [
    "seq_len = 10\n",
    "batch_size = 16\n",
    "embed_size = 16\n",
    "vocab_size = len(char2int)\n",
    "hidden_size = 64\n",
    "lr = 0.001\n",
    "\n",
    "model = CharRNN(seq_len=seq_len, vocab_size=vocab_size, \n",
    "                embed_size=embed_size, hidden_size=hidden_size, batch_size=batch_size)                \n",
    "                \n",
    "optimizer = optim.Adam(model.parameters(), lr=lr)\n",
    "criterion = nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "epoch=0 | elapsed=17.2 | Loss: 2.248 | Perplexity: 8.476 \n",
      "epoch=1 | elapsed=16.9 | Loss: 1.948 | Perplexity: 7.923 \n",
      "epoch=2 | elapsed=16.9 | Loss: 1.865 | Perplexity: 7.971 \n",
      "epoch=3 | elapsed=16.9 | Loss: 1.816 | Perplexity: 7.704 \n",
      "epoch=4 | elapsed=17.0 | Loss: 1.781 | Perplexity: 7.943 \n",
      "epoch=5 | elapsed=17.5 | Loss: 1.754 | Perplexity: 7.734 \n",
      "epoch=6 | elapsed=17.3 | Loss: 1.733 | Perplexity: 7.728 \n",
      "epoch=7 | elapsed=17.4 | Loss: 1.715 | Perplexity: 7.310 \n",
      "epoch=8 | elapsed=16.9 | Loss: 1.701 | Perplexity: 7.676 \n",
      "epoch=9 | elapsed=17.3 | Loss: 1.689 | Perplexity: 7.533 \n",
      "epoch=10 | elapsed=17.0 | Loss: 1.678 | Perplexity: 7.402 \n",
      "epoch=11 | elapsed=16.8 | Loss: 1.669 | Perplexity: 7.629 \n",
      "epoch=12 | elapsed=17.0 | Loss: 1.660 | Perplexity: 7.547 \n",
      "epoch=13 | elapsed=17.1 | Loss: 1.653 | Perplexity: 7.579 \n",
      "epoch=14 | elapsed=18.0 | Loss: 1.646 | Perplexity: 7.356 \n",
      "epoch=15 | elapsed=17.5 | Loss: 1.641 | Perplexity: 7.282 \n",
      "epoch=16 | elapsed=16.7 | Loss: 1.634 | Perplexity: 7.357 \n",
      "epoch=17 | elapsed=16.7 | Loss: 1.629 | Perplexity: 7.470 \n",
      "epoch=18 | elapsed=16.8 | Loss: 1.625 | Perplexity: 7.283 \n",
      "epoch=19 | elapsed=17.1 | Loss: 1.620 | Perplexity: 7.209 \n",
      "epoch=20 | elapsed=17.0 | Loss: 1.616 | Perplexity: 7.462 \n"
     ]
    }
   ],
   "source": [
    "start = time.time()\n",
    "n_epochs = 20\n",
    "print_every = 1\n",
    "\n",
    "for epoch in range(n_epochs + 1):\n",
    "    old_loss = loss\n",
    "    train_iterator = iter_names(b_train, batch_size, seq_len, vocab_size)\n",
    "    loss = train(model, train_iterator, optimizer, criterion)\n",
    "    test_iterator = iter_names(b_test, batch_size, seq_len, vocab_size)\n",
    "    pp = perplexity(model, test_iterator)\n",
    "    if epoch % print_every == 0:\n",
    "        elapsed = time.time() - start\n",
    "        print(f'epoch={epoch} | elapsed={elapsed:.1f} | Loss: {loss:.3f} | Perplexity: {pp:.3f} ')  \n",
    "        start = time.time()\n",
    "        torch.save(model, \"rnn1.pt\")\n",
    "    if np.abs(loss - old_loss) < 0.001:\n",
    "        break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Формирование итератора батчей\n",
    "\n",
    "# Выравнивает строки\n",
    "def neq(name, seq_len):\n",
    "    n_len = len(name)\n",
    "    if n_len > seq_len:\n",
    "        name = name[0:seq_len]\n",
    "    elif n_len < seq_len:\n",
    "        name += ' ' * (seq_len - n_len)\n",
    "    return name     \n",
    "\n",
    "# Выделяет +1 символ\n",
    "def y_char(name, end='<EOS>'):\n",
    "    out = ''\n",
    "    for i, char in enumerate(name):\n",
    "        a = end\n",
    "        if i < len(name) - 1:\n",
    "            a = name[i+1]\n",
    "        out = out + str(a)\n",
    "    return out\n",
    "\n",
    "def iter_names(arr, batch_size, seq_len, vocab_size):\n",
    "    '''Create a generator that returns batches of size\n",
    "       batch_size х seq_len x vocab_size from _shuffled_ list arr.\n",
    "    '''\n",
    "# Перемешивание списка перед отбрасыванием последнего батча    \n",
    "    arr = shuffle(arr)\n",
    "    num_batches = len(arr) // batch_size\n",
    "    i = -1\n",
    "    for _ in range(num_batches):\n",
    "        for n in range(batch_size):\n",
    "            i += 1\n",
    "# Выравнивание        \n",
    "            name = neq(arr[i], seq_len)\n",
    "# Кодирование\n",
    "            encoded = np.array([char2int[char] for char in name])\n",
    "            encoded = np.expand_dims(encoded, axis=0)            \n",
    "            y_enc = np.array([char2int[char] for char in y_char(name, ' ')])\n",
    "            y_enc = np.expand_dims(y_enc, axis=0)            \n",
    "        \n",
    "            if n == 0:\n",
    "                batch = encoded\n",
    "                y = y_enc\n",
    "            else:\n",
    "                batch = np.vstack([batch, encoded])\n",
    "                y = np.vstack([y, y_enc])                \n",
    "\n",
    "        x_train = torch.from_numpy(batch).type(torch.LongTensor)\n",
    "        y_train = torch.from_numpy(y).type(torch.LongTensor)\n",
    "        \n",
    "        yield x_train, y_train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
