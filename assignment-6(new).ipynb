{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Отчет о проделанной работе\n",
    "\n",
    "## Результаты\n",
    "\n",
    " - Достигнута точность 68.66%, что меньше требуемой 70%.\n",
    " - Учет поверхностных признаков повысил точность на 0.66%, хотя сами по себе они дают точность примерно 58%."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import time\n",
    "import logging\n",
    "logging.basicConfig(filename=\"pt.log\", level=logging.INFO)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn import metrics\n",
    "from sklearn.utils import shuffle\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.svm import LinearSVC\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "from torchtext.data import Field, LabelField, BucketIterator, TabularDataset, Iterator\n",
    "\n",
    "import nltk\n",
    "import gensim\n",
    "import spacy\n",
    "import en_core_web_sm\n",
    "spacy_en = en_core_web_sm.load()\n",
    "\n",
    "def tokenizer(text):\n",
    "    return [tok.text for tok in spacy_en.tokenizer(text) if tok.text.isalpha()]     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv('sarcasm.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = shuffle(df)\n",
    "df[:800000].to_csv('train6.csv', index=True, index_label='index')\n",
    "df[800000:].to_csv('test6.csv', index=True, index_label='index')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "TRAIN = 'train6.csv'\n",
    "TEST = 'test6.csv'\n",
    "FREQ = 50\n",
    "batch_size = 32\n",
    "\n",
    "TEXT = Field(include_lengths=True, batch_first=True, \n",
    "             tokenize=tokenizer,\n",
    "             eos_token='<eos>',\n",
    "             lower=True,\n",
    "             stop_words=nltk.corpus.stopwords.words('english'))\n",
    "\n",
    "LABEL = LabelField(dtype=torch.int64)\n",
    "\n",
    "INDEX = Field(sequential=False, use_vocab=False, dtype=torch.int64)\n",
    "\n",
    "d_train = TabularDataset(TRAIN, format='csv', \n",
    "    fields=[('index', INDEX), ('label', LABEL), ('text', TEXT)], skip_header=True)\n",
    "\n",
    "d_test = TabularDataset(TEST, format='csv', \n",
    "    fields=[('index', INDEX), ('label', LABEL), ('text', TEXT)], skip_header=True)\n",
    "\n",
    "TEXT.build_vocab(d_train, min_freq=FREQ, vectors=\"glove.6B.100d\")\n",
    "LABEL.build_vocab(d_train)\n",
    "\n",
    "train_iterator = BucketIterator(d_train, batch_size, shuffle=True,\n",
    "    sort_key=lambda x: len(x.text))\n",
    "\n",
    "test_iterator = BucketIterator(d_test, batch_size, shuffle=True,\n",
    "    sort_key=lambda x: len(x.text))\n",
    "\n",
    "pretrained_embeddings = TEXT.vocab.vectors\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNN(nn.Module):\n",
    "    def __init__(self, vocab_size, embedding_dim, n_filters, filter_sizes, output_dim, dropout):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
    "        self.conv_0 = nn.Conv1d(in_channels=1, out_channels=n_filters, \n",
    "                                kernel_size=(filter_sizes[0], embedding_dim))\n",
    "        self.conv_1 = nn.Conv1d(in_channels=1, out_channels=n_filters,\n",
    "                        kernel_size=(filter_sizes[1], embedding_dim))        \n",
    "        self.conv_2 = nn.Conv1d(in_channels=1, out_channels=n_filters,\n",
    "                        kernel_size=(filter_sizes[2], embedding_dim))        \n",
    "        self.conv_3 = nn.Conv1d(in_channels=1, out_channels=n_filters,\n",
    "                                kernel_size=(filter_sizes[3], embedding_dim))        \n",
    "        self.fc1 = nn.Linear(len(filter_sizes)*n_filters, 100)\n",
    "        \n",
    "        self.dropout = nn.Dropout(dropout)\n",
    "        self.fc2 = nn.Linear(100, 20)\n",
    "        self.fc3 = nn.Linear(20, 1)\n",
    "        \n",
    "    def forward(self, x):\n",
    "        embedded = self.embedding(x)\n",
    "        embedded = embedded.unsqueeze(1)\n",
    "        conved_0 = F.relu(self.conv_0(embedded).squeeze(3))\n",
    "        conved_1 = F.relu(self.conv_1(embedded).squeeze(3))\n",
    "        conved_2 = F.relu(self.conv_2(embedded).squeeze(3))\n",
    "        conved_3 = F.relu(self.conv_3(embedded).squeeze(3))        \n",
    "        pooled_0 = F.max_pool1d(conved_0, conved_0.shape[2]).squeeze(2)\n",
    "        pooled_1 = F.max_pool1d(conved_1, conved_1.shape[2]).squeeze(2)\n",
    "        pooled_2 = F.max_pool1d(conved_2, conved_2.shape[2]).squeeze(2)        \n",
    "        pooled_3 = F.max_pool1d(conved_3, conved_3.shape[2]).squeeze(2)                \n",
    "        cat = self.dropout(torch.cat((pooled_0, pooled_1, pooled_2, pooled_3), dim=1))\n",
    "        x = self.fc1(cat)\n",
    "        x = self.fc2(x)\n",
    "        x = self.fc3(x)\n",
    "        return x\n",
    "    \n",
    "INPUT_DIM = len(TEXT.vocab)\n",
    "EMBEDDING_DIM = 100\n",
    "N_FILTERS = 100\n",
    "FILTER_SIZES = [2,3,4,5]\n",
    "\n",
    "OUTPUT_DIM = 1\n",
    "\n",
    "DROPOUT = 0.5\n",
    "print(INPUT_DIM, OUTPUT_DIM)\n",
    "\n",
    "model = CNN(INPUT_DIM, EMBEDDING_DIM, N_FILTERS, \n",
    "                                FILTER_SIZES, OUTPUT_DIM, DROPOUT)\n",
    "# +GLOVE\n",
    "model.embedding.weight.data.copy_(pretrained_embeddings)\n",
    "# +FROZEN\n",
    "model.embedding.weight.requires_grad=False\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def accuracy(y_pred, y_true):\n",
    "    y_true = y_true.byte()\n",
    "    y_pred = torch.ge(y_pred, 0.5)\n",
    "    return torch.mean(torch.eq(y_pred, y_true).float())\n",
    "\n",
    "def bb5(t0):\n",
    "    s = t0.size()[1]\n",
    "    if s == 4:\n",
    "        t0 = F.pad(t0, (0, 1), 'constant', 1)\n",
    "    elif s == 3:\n",
    "        t0 = F.pad(t0, (0, 1), 'constant', 1)\n",
    "        t0 = F.pad(t0, (0, 1), 'constant', 1)\n",
    "    elif s == 2:        \n",
    "        t0 = F.pad(t0, (0, 1), 'constant', 1)\n",
    "        t0 = F.pad(t0, (0, 1), 'constant', 1)\n",
    "        t0 = F.pad(t0, (0, 1), 'constant', 1)        \n",
    "    elif s == 1:        \n",
    "        t0 = F.pad(t0, (0, 1), 'constant', 1)\n",
    "        t0 = F.pad(t0, (0, 1), 'constant', 1)\n",
    "        t0 = F.pad(t0, (0, 1), 'constant', 1)        \n",
    "        t0 = F.pad(t0, (0, 1), 'constant', 1)                \n",
    "    return t0\n",
    "\n",
    "def train(model, iterator, optimizer, criterion):\n",
    "\n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "\n",
    "    model.train()\n",
    "    i = 0\n",
    "    for batch in iterator:\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        text = bb5(batch.text[0])\n",
    "        y_train = batch.label.float()  \n",
    "        \n",
    "        i += 1\n",
    "        logging.info(str(epoch) + ' ' + str(i) + ' ' + str(text.size()))\n",
    "       \n",
    "        pred = model(text)        \n",
    "        y_pred = F.sigmoid(pred)        \n",
    "        y_pred = y_pred.squeeze()\n",
    "        loss = criterion(y_pred, y_train)\n",
    "        acc = accuracy(y_pred, y_train)\n",
    "        \n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        epoch_loss += loss.item()\n",
    "        epoch_acc += acc.item()\n",
    "\n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)\n",
    "\n",
    "def evaluate(model, iterator, criterion):\n",
    "    \n",
    "    epoch_loss = 0\n",
    "    epoch_acc = 0\n",
    "    model.eval()\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        \n",
    "        i = 0\n",
    "        for batch in iterator:\n",
    "            i += 1\n",
    "            text = bb5(batch.text[0])\n",
    "            target = batch.label        \n",
    "            y_test = batch.label.float()  \n",
    "            ind = batch.index\n",
    "            \n",
    "            predictions = model(text)\n",
    "            prob = F.sigmoid(predictions)\n",
    "            \n",
    "            pred = model(text)        \n",
    "            y_pred = F.sigmoid(pred)            \n",
    "            y_pred = y_pred.squeeze()\n",
    "            loss = criterion(y_pred, y_test)\n",
    "            acc = accuracy(y_pred, y_test)\n",
    "            \n",
    "            epoch_loss += loss.item()\n",
    "            epoch_acc += acc.item()\n",
    "        \n",
    "    return epoch_loss / len(iterator), epoch_acc / len(iterator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "len_train= 25000\n",
      "len_test= 6589\n",
      "Epoch: 01 | Train Loss: 0.581 | Train Acc: 69.02%\n",
      "elapsed=772.6 | Test Loss: 0.593 | Test Acc: 67.82%\n",
      "Epoch: 02 | Train Loss: 0.579 | Train Acc: 69.19%\n",
      "elapsed=761.5 | Test Loss: 0.592 | Test Acc: 67.93%\n",
      "Epoch: 03 | Train Loss: 0.578 | Train Acc: 69.30%\n",
      "elapsed=761.5 | Test Loss: 0.592 | Test Acc: 68.00%\n",
      "Epoch: 04 | Train Loss: 0.576 | Train Acc: 69.47%\n",
      "elapsed=766.9 | Test Loss: 0.593 | Test Acc: 67.90%\n",
      "Epoch: 05 | Train Loss: 0.575 | Train Acc: 69.46%\n",
      "elapsed=762.7 | Test Loss: 0.591 | Test Acc: 68.07%\n",
      "Epoch: 06 | Train Loss: 0.573 | Train Acc: 69.63%\n",
      "elapsed=767.8 | Test Loss: 0.592 | Test Acc: 67.96%\n",
      "Epoch: 07 | Train Loss: 0.572 | Train Acc: 69.77%\n",
      "elapsed=766.2 | Test Loss: 0.593 | Test Acc: 68.09%\n",
      "Epoch: 08 | Train Loss: 0.571 | Train Acc: 69.86%\n",
      "elapsed=767.9 | Test Loss: 0.593 | Test Acc: 68.08%\n",
      "Epoch: 09 | Train Loss: 0.570 | Train Acc: 69.89%\n",
      "elapsed=764.5 | Test Loss: 0.593 | Test Acc: 68.03%\n",
      "Epoch: 10 | Train Loss: 0.569 | Train Acc: 69.99%\n",
      "elapsed=764.3 | Test Loss: 0.591 | Test Acc: 68.18%\n",
      "Epoch: 11 | Train Loss: 0.568 | Train Acc: 70.17%\n",
      "elapsed=766.2 | Test Loss: 0.592 | Test Acc: 68.09%\n",
      "Epoch: 12 | Train Loss: 0.567 | Train Acc: 70.10%\n",
      "elapsed=765.3 | Test Loss: 0.592 | Test Acc: 68.02%\n",
      "Epoch: 13 | Train Loss: 0.566 | Train Acc: 70.24%\n",
      "elapsed=764.7 | Test Loss: 0.592 | Test Acc: 68.01%\n",
      "Epoch: 14 | Train Loss: 0.565 | Train Acc: 70.34%\n",
      "elapsed=766.2 | Test Loss: 0.593 | Test Acc: 68.00%\n",
      "Epoch: 15 | Train Loss: 0.564 | Train Acc: 70.38%\n",
      "elapsed=769.0 | Test Loss: 0.592 | Test Acc: 68.02%\n",
      "Epoch: 16 | Train Loss: 0.564 | Train Acc: 70.37%\n",
      "elapsed=764.9 | Test Loss: 0.592 | Test Acc: 68.00%\n",
      "Epoch: 17 | Train Loss: 0.563 | Train Acc: 70.47%\n",
      "elapsed=767.2 | Test Loss: 0.592 | Test Acc: 68.14%\n",
      "Epoch: 18 | Train Loss: 0.562 | Train Acc: 70.54%\n",
      "elapsed=765.5 | Test Loss: 0.592 | Test Acc: 68.08%\n",
      "Epoch: 19 | Train Loss: 0.561 | Train Acc: 70.62%\n",
      "elapsed=767.0 | Test Loss: 0.592 | Test Acc: 68.04%\n",
      "Epoch: 20 | Train Loss: 0.560 | Train Acc: 70.67%\n",
      "elapsed=766.7 | Test Loss: 0.596 | Test Acc: 68.06%\n",
      "Epoch: 21 | Train Loss: 0.560 | Train Acc: 70.68%\n",
      "elapsed=766.4 | Test Loss: 0.595 | Test Acc: 68.04%\n",
      "Epoch: 22 | Train Loss: 0.560 | Train Acc: 70.74%\n",
      "elapsed=764.8 | Test Loss: 0.594 | Test Acc: 68.09%\n",
      "Epoch: 23 | Train Loss: 0.559 | Train Acc: 70.77%\n",
      "elapsed=766.2 | Test Loss: 0.596 | Test Acc: 67.93%\n",
      "Epoch: 24 | Train Loss: 0.558 | Train Acc: 70.84%\n",
      "elapsed=766.5 | Test Loss: 0.595 | Test Acc: 68.06%\n",
      "Epoch: 25 | Train Loss: 0.558 | Train Acc: 70.92%\n",
      "elapsed=763.4 | Test Loss: 0.598 | Test Acc: 67.92%\n",
      "Epoch: 26 | Train Loss: 0.557 | Train Acc: 70.97%\n",
      "elapsed=764.1 | Test Loss: 0.593 | Test Acc: 68.05%\n",
      "Epoch: 27 | Train Loss: 0.557 | Train Acc: 71.00%\n",
      "elapsed=764.5 | Test Loss: 0.594 | Test Acc: 68.12%\n",
      "Epoch: 28 | Train Loss: 0.556 | Train Acc: 71.03%\n",
      "elapsed=768.0 | Test Loss: 0.602 | Test Acc: 68.08%\n",
      "Epoch: 29 | Train Loss: 0.555 | Train Acc: 71.10%\n",
      "elapsed=763.8 | Test Loss: 0.593 | Test Acc: 68.11%\n",
      "Epoch: 30 | Train Loss: 0.555 | Train Acc: 71.06%\n",
      "elapsed=764.9 | Test Loss: 0.594 | Test Acc: 68.07%\n",
      "Epoch: 31 | Train Loss: 0.553 | Train Acc: 71.20%\n",
      "elapsed=767.0 | Test Loss: 0.593 | Test Acc: 67.98%\n",
      "Epoch: 32 | Train Loss: 0.554 | Train Acc: 71.20%\n",
      "elapsed=765.4 | Test Loss: 0.594 | Test Acc: 67.95%\n",
      "Epoch: 33 | Train Loss: 0.554 | Train Acc: 71.20%\n",
      "elapsed=767.9 | Test Loss: 0.595 | Test Acc: 68.02%\n",
      "Epoch: 34 | Train Loss: 0.553 | Train Acc: 71.21%\n",
      "elapsed=768.7 | Test Loss: 0.595 | Test Acc: 68.03%\n",
      "Epoch: 35 | Train Loss: 0.552 | Train Acc: 71.31%\n",
      "elapsed=762.0 | Test Loss: 0.595 | Test Acc: 67.94%\n",
      "Epoch: 36 | Train Loss: 0.552 | Train Acc: 71.29%\n",
      "elapsed=762.1 | Test Loss: 0.596 | Test Acc: 68.06%\n",
      "Epoch: 37 | Train Loss: 0.552 | Train Acc: 71.31%\n",
      "elapsed=768.4 | Test Loss: 0.599 | Test Acc: 68.14%\n",
      "Epoch: 38 | Train Loss: 0.552 | Train Acc: 71.35%\n",
      "elapsed=768.7 | Test Loss: 0.601 | Test Acc: 68.03%\n",
      "Epoch: 39 | Train Loss: 0.551 | Train Acc: 71.44%\n",
      "elapsed=765.4 | Test Loss: 0.595 | Test Acc: 68.09%\n",
      "Epoch: 40 | Train Loss: 0.550 | Train Acc: 71.47%\n",
      "elapsed=767.7 | Test Loss: 0.596 | Test Acc: 68.07%\n",
      "best_acc= 0.6818124905153985 epoch_best= 10\n"
     ]
    }
   ],
   "source": [
    "optimizer = optim.Adam(model.parameters())\n",
    "criterion = nn.BCELoss()\n",
    "N_EPOCHS = 40\n",
    "BEST = 0\n",
    "EPOCH = 0\n",
    "\n",
    "for epoch in range(N_EPOCHS):\n",
    "    start = time.time()        \n",
    "    train_loss, train_acc = train(model, train_iterator, optimizer, criterion)\n",
    "    \n",
    "    print(f'Epoch: {epoch+1:02} | Train Loss: {train_loss:.3f} | Train Acc: {train_acc*100:.2f}%')\n",
    "    \n",
    "    test_loss, test_acc = evaluate(model, test_iterator, criterion)\n",
    "    elapsed = time.time() - start\n",
    "    print(f'elapsed={elapsed:.1f} | Test Loss: {test_loss:.3f} | Test Acc: {test_acc*100:.2f}%')  \n",
    "    \n",
    "    if test_acc > BEST:\n",
    "        BEST = test_acc\n",
    "        EPOCH = epoch + 1\n",
    "        torch.save(model, \"t6.pt\")\n",
    "\n",
    "print('best_acc=', BEST, 'epoch_best=', EPOCH)        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  },
  "widgets": {
   "state": {},
   "version": "1.1.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
